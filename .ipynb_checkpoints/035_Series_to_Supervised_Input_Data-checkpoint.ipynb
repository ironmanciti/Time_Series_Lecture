{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series 를 DNN 용의 지도학습 dataset 으로 전환\n",
    "\n",
    "## tf.data.Dataset\n",
    "\n",
    "    - range(start, stop, step) - step 단계로 구분된 범위 값의 데이터 집합을 생성\n",
    "    - window(size, shift) - input element를 윈도우 데이터셋으로 결합\n",
    "    - flat_map(map_func) - 이 데이터세트 전체에 map_func를 매핑하고 그 결과를 flatten \n",
    "    - batch - 이 데이터 세트의 연속 elements를 batch로 결합\n",
    "    - map - 이 데이터세트의 element 전체에 map_func를 매핑\n",
    "    - shuffle - 이 데이터 세트의 elements를 무작위로 섞습니다.\n",
    "    - prefetch(buffer_size) - buffer_size batch를 prefetch\n",
    "    - from_tensor_slices - elements가 주어진 텐서의 조각인 Dataset을 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `range(start, stop, step)`\n",
    "- step 단계로 구분된 범위 값의 데이터 집합을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.range(10)\n",
    "\n",
    "for val in ds:\n",
    "    print(val.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## window(size, shift)\n",
    "- 1 element씩 shift하며, input element를 윈도우 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01234\n",
      "12345\n",
      "23456\n",
      "34567\n",
      "45678\n",
      "56789\n",
      "6789\n",
      "789\n",
      "89\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.range(10)\n",
    "ds = ds.window(5, shift=1)\n",
    "\n",
    "for window_ds in ds:\n",
    "    for val in window_ds:\n",
    "        print(val.numpy(), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ds.take(count)\n",
    "\n",
    "- count 갯수만큼 취함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "22\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "for sales in ds.take(3):\n",
    "    print(sales.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter(filter_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "22\n",
      "31\n",
      "32\n",
      "34\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "ds = ds.filter(lambda x: x > 0)\n",
    "\n",
    "for sales in ds:\n",
    "    print(sales.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map\n",
    "\n",
    "- Dataset 전체에 함수를 맵핑합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "44\n",
      "62\n",
      "64\n",
      "68\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(lambda x: x * 2)\n",
    "\n",
    "for sales in ds:\n",
    "    print(sales.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## window(size, shift, drop_remainder=True)\n",
    "\n",
    "- ds.window : window dataset 생성\n",
    "\n",
    "    - window: 그룹화 할 window size  \n",
    "    - drop_remainder: 남은 부분을 버릴지 살릴지 여부  \n",
    "    - shift는 1 iteration당 몇 element씩 이동할 것인지  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01234\n",
      "12345\n",
      "23456\n",
      "34567\n",
      "45678\n",
      "56789\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.range(10)\n",
    "ds = ds.window(5, shift=1, drop_remainder=True)\n",
    "\n",
    "for window_ds in ds:\n",
    "    for w in window_ds:\n",
    "        print(w.numpy(), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flat_map(map_func)\n",
    "- flat_map은 dataset에 함수를 apply해주고, 결과를 flatten하게 펼쳐 줍니다.\n",
    "\n",
    "- 아래는 lambda 함수를 통해 5개의 batch를 읽어들인 뒤 flatten된 리턴값을 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "[1 2 3 4 5]\n",
      "[2 3 4 5 6]\n",
      "[3 4 5 6 7]\n",
      "[4 5 6 7 8]\n",
      "[5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.range(10)\n",
    "ds = ds.window(5, shift=1, drop_remainder=True)\n",
    "\n",
    "# window element들을 하나의 list로 결합\n",
    "ds = ds.flat_map(lambda w: w.batch(5))   \n",
    "for window_ds in ds:\n",
    "    print(window_ds.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ds.flat_map(lambda w: w.batch(5))은 아래의 coding 과 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "[1 2 3 4 5]\n",
      "[2 3 4 5 6]\n",
      "[3 4 5 6 7]\n",
      "[4 5 6 7 8]\n",
      "[5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.range(10)\n",
    "ds = ds.window(5, shift=1, drop_remainder=True)\n",
    "\n",
    "for window_ds in ds:\n",
    "    tmp = []\n",
    "    for w in window_ds:\n",
    "        tmp.append(w.numpy()) # window element들을 하나의 list로 결합\n",
    "    print(np.array(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature / label 분할\n",
    "\n",
    "- Time Series Dataset을 만드려는 경우, train/label 값을 분류하는 용도로 `map()`을 활용할 수 있습니다.\n",
    "\n",
    "- `x[:-1]`, `x[-1:]` 의 의도는 각 row의 마지막 index 전까지는 train data로, 마지막 index는 label로 활용하겠다는 의도입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3] [4]\n",
      "[1 2 3 4] [5]\n",
      "[2 3 4 5] [6]\n",
      "[3 4 5 6] [7]\n",
      "[4 5 6 7] [8]\n",
      "[5 6 7 8] [9]\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.range(10)\n",
    "ds = ds.window(5, shift=1, drop_remainder=True)\n",
    "ds = ds.flat_map(lambda w: w.batch(5))   \n",
    "ds = ds.map(lambda w: (w[:-1], w[-1:]))\n",
    "\n",
    "for x, y in ds:\n",
    "    print(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shuffle(buffer_size)\n",
    "\n",
    "- 데이터세트는 buffer_size elements로 버퍼를 채운 다음이 버퍼에서 elements를 무작위로 샘플링하여 선택한 elements를 새 elements로 바꿉니다.\n",
    "\n",
    "- buffer_size: 새 데이터 세트가 샘플링할 이 데이터 세트의 element 수\n",
    "\n",
    "- 완벽한 셔플 링을 위해서는 데이터 세트의 전체 크기보다 크거나 같은 버퍼 크기가 필요합니다.\n",
    "\n",
    "- 예를 들어, 데이터 집합에 10,000 개의 element가 있지만 buffer_size가 1,000으로 설정된 경우 셔플은 처음에 버퍼의 처음 1,000 개 element 중 임의의 element 만 선택합니다.\n",
    "\n",
    "- element가 선택되면 버퍼의 공간이 다음 element (즉, 1,001번째)로 대체되어 1,000 element 버퍼를 유지합니다.\n",
    "\n",
    "- shuffle 의 위치는 input, label 분리 이전, 이후 모두 무방\n",
    "\n",
    "### Sequence bias (시퀀스 편향)\n",
    "\n",
    "- 시퀀스 편향은 시장 조사에서 목록의 특정 순서로 인해 사물, 사람 또는 그룹에 표시되는 편견 또는 호의로 정의됩니다. 예를 들어 응답자에게 가장 좋아하는 음식이 무엇이고 피자가 항상 햄버거와 핫도그보다 먼저 순서대로 나열되어 있는 경우 응답자는 피자를 가장 먼저 읽었기 때문에 피자를 선택하는 경향이 더 높을 수 있습니다. 표시된 마지막 음식(핫도그)에 대해서도 마찬가지입니다. 이 경우 해당 음식이 목록의 마지막에 있기 때문에 선택될 가능성이 가장 낮을 수 있습니다.  \n",
    "\n",
    "\n",
    "- 이러한 문제를 방지하기 위해 dataset을 shuffle 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4] [5]\n",
      "[5 6 7 8] [9]\n",
      "[2 3 4 5] [6]\n",
      "[3 4 5 6] [7]\n",
      "[4 5 6 7] [8]\n",
      "[0 1 2 3] [4]\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.range(10)\n",
    "ds = ds.window(5, shift=1, drop_remainder=True)\n",
    "ds = ds.flat_map(lambda w: w.batch(5))   \n",
    "ds = ds.map(lambda w: (w[:-1], w[-1:]))\n",
    "ds = ds.shuffle(buffer_size=10)  # data shuffle\n",
    "\n",
    "for x, y in ds:\n",
    "    print(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prefetch(buffer_size) \n",
    "- GPU memory 상으로 buffer_size batch를 prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[3 4 5 6]\n",
      " [0 1 2 3]]\n",
      "y =  [[7]\n",
      " [4]]\n",
      "x =  [[4 5 6 7]\n",
      " [2 3 4 5]]\n",
      "y =  [[8]\n",
      " [6]]\n",
      "x =  [[1 2 3 4]\n",
      " [5 6 7 8]]\n",
      "y =  [[5]\n",
      " [9]]\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.range(10)\n",
    "ds = ds.window(5, shift=1, drop_remainder=True)\n",
    "ds = ds.flat_map(lambda w: w.batch(5))  \n",
    "ds = ds.map(lambda w: (w[:-1], w[-1:]))\n",
    "ds = ds.shuffle(buffer_size=10)\n",
    "ds = ds.batch(2).prefetch(1)\n",
    "\n",
    "for x, y in ds:\n",
    "    print(\"x = \", x.numpy())\n",
    "    print(\"y = \", y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## repeat \n",
    "\n",
    "- 반복할 때마다 데이터셋을 다시 초기화\n",
    "- element의 무한 시퀀스를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3) (1, 1) (2, 5) (3, 4) (4, 2) (5, 6) (6, 3) (7, 2) (8, 1) (9, 6) (10, 5) "
     ]
    }
   ],
   "source": [
    "arr = [1, 2, 3, 4, 5, 6]\n",
    "ds = tf.data.Dataset.from_tensor_slices(arr)\n",
    "ds = ds.shuffle(buffer_size=3).repeat()\n",
    "\n",
    "for idx, elem in enumerate(ds):\n",
    "    print((idx, elem.numpy()), end=\" \")\n",
    "    if idx >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `ds.repeat().batch()` 가 생성하는 것\n",
    "\n",
    "- 배치 전에 `ds.repeat()`가 있으므로 데이터 생성이 계속됩니다. 그러나 batch내 element의 순서는 `ds.random()`으로 인해 달라집니다. 고려해야 할 점은 랜덤 버퍼의 크기로 인해 6이 첫 번째 배치에 존재하지 않는다는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [3 2 4]\n",
      "1 [6 5 1]\n",
      "2 [3 2 4]\n",
      "3 [6 5 1]\n",
      "4 [3 1 2]\n",
      "5 [6 4 5]\n"
     ]
    }
   ],
   "source": [
    "arr = [1, 2, 3, 4, 5, 6]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(arr)\n",
    "dataset = dataset.shuffle(buffer_size=3).repeat().batch(3)\n",
    "\n",
    "for idx, elem in enumerate(dataset):\n",
    "  print(idx, elem.numpy())\n",
    "  if idx >= 5:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure Python Code 와 tf.data.Dataset 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순수 Python code 로 지도학습 data 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(series, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series)-window_size):\n",
    "        seq_x, seq_y = series[i:i+window_size], series[i+window_size]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3] 4\n",
      "[1 2 3 4] 5\n",
      "[2 3 4 5] 6\n",
      "[3 4 5 6] 7\n",
      "[4 5 6 7] 8\n",
      "[5 6 7 8] 9\n"
     ]
    }
   ],
   "source": [
    "data = [i for i in range(10)]\n",
    "X, y = series_to_supervised(data, 4)\n",
    "\n",
    "for i in range(len(X)):\n",
    "     print(X[i], y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.data.Dataset 을 이용한 windowed dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size+1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size+1))\n",
    "    ds = ds.map(lambda w: (w[:-1], w[-1]))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3] 4\n",
      "[1 2 3 4] 5\n",
      "[2 3 4 5] 6\n",
      "[3 4 5 6] 7\n",
      "[4 5 6 7] 8\n",
      "[5 6 7 8] 9\n"
     ]
    }
   ],
   "source": [
    "data = [i for i in range(10)]\n",
    "ds = windowed_dataset(data, 4, 1, 10)\n",
    "\n",
    "for x, y in ds:\n",
    "    print(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
